{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark assignment 2: Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the second part of the assignment, your task is to extract collocations: that is word combinations that occur together. For example, “high school” or “roman empire”.\n",
    "\n",
    "To find collocations, you will use NPMI (normalized pointwise mutual information) metric.\n",
    "\n",
    "PMI of two words, a & b, is defined as “PMI(a, b) = ln (P(ab) / (P(a) * P(b))”, where P(ab) is the probability of two words coming one after the other, and P(a) and P(b) are probabilities of words a & b respectively.\n",
    "\n",
    "You will estimate probabilities with occurrence counts, that is “P(a) = # of occurrences of word a / total number of words”, and “P(ab) = # of occurrences of words ‘a b’ / total number of word pairs”.\n",
    "\n",
    "To build an intuition behind the definition, see Reading material.\n",
    "\n",
    "Therefore, rare combinations of coupled words have large PMI.\n",
    "\n",
    "NPMI is computed as “NPMI(a, b) = PMI(a, b) / -ln P(ab)”. This normalizes the quantity to be within the range [-1; 1].\n",
    "\n",
    "You task is a bit more complicated now:\n",
    "\n",
    "- Extract all the words, as in the previous task.\n",
    "- Filter out stopwords using the dictionary (/datasets/stop_words_en.txt ) (do not forget to convert words to the lowercase!)\n",
    "- Compute all bigrams (that is, pairs of consequent words)\n",
    "- Leave only bigrams with at least 500 occurrences\n",
    "- Compute NPMI for every bigram (note: when computing probabilities, you need unpruned counts!)\n",
    "- Sort word pairs by NPMI in the descending order\n",
    "- Print top 39 word pairs, with words delimited by the underscore “_”\n",
    "\n",
    "For example,\n",
    "\n",
    "    roman_empire\n",
    "    south_africa\n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "    ...\n",
    "    references_reading\n",
    "    notes_references\n",
    "    award_best\n",
    "    north_america\n",
    "    new_zealand\n",
    "    ...\n",
    " \n",
    "Hint: if you did everything right, “roman_empire” and “south_africa” are going to be in the result.\n",
    "\n",
    "If you want to deploy the environment on your own machine, please use bigdatateam/spark-course1 Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext(conf=SparkConf().setAppName(\"MyApp\").setMaster(\"local[2]\"))\n",
    "\n",
    "import re\n",
    "import math\n",
    "\n",
    "stop_file = \"/datasets/stop_words_en.txt\"\n",
    "wiki_file = \"/data/wiki/en_articles_part/articles-part\"\n",
    "pair_thresh = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a=sc.parallelize([1, 2, 3, 4, 5])\n",
    "#a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stop_file, \"r\") as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "    \n",
    "stop_words_bcast = sc.broadcast(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(line):\n",
    "    try:\n",
    "        article_id, text = line.rstrip().split('\\t', 1)\n",
    "        text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "        words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "        return words\n",
    "    except ValueError as e:\n",
    "        return []\n",
    "    \n",
    "def lower(words):\n",
    "    return [word.lower() for word in words]\n",
    "\n",
    "def filter_stop(words):\n",
    "    return [word for word in words if word not in stop_words_bcast.value]\n",
    "\n",
    "def pairs(words):\n",
    "    out = []\n",
    "    for w1, w2 in zip(words[:-1], words[1:]):\n",
    "        out.append((w1.lower() + \"_\" + w2.lower(), 1))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = (sc.textFile(wiki_file, 16)\n",
    "         .map(parse_article) \n",
    "         .map(lower)\n",
    "         .map(filter_stop)\n",
    "        ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100\n"
     ]
    }
   ],
   "source": [
    "print wiki.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = (wiki.flatMap(lambda wds : [(word, 1) for word in wds])\n",
    "         .reduceByKey(lambda x,y : x+y)\n",
    "        ).cache()\n",
    "\n",
    "pairs = (wiki.flatMap(pairs)\n",
    "         .reduceByKey(lambda x,y : x+y)\n",
    "        ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biennials\t10\n",
      "underlyingly\t1\n",
      "ancyra\t43\n",
      "tripolitan\t2\n",
      "tilton\t4\n",
      "nordland\t1\n",
      "squealer\t8\n",
      "regularize\t2\n",
      "skylights.passive\t1\n",
      "thesis\"(kleene\t1\n"
     ]
    }
   ],
   "source": [
    "for key, count in words.take(10):\n",
    "    print(\"%s\\t%d\" % (key, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'2,000_1.5', 1),\n",
       " (u'fastest_mode', 1),\n",
       " (u'cases_federal', 4),\n",
       " (u'creem_particular', 1),\n",
       " (u'subgroups_lie', 2),\n",
       " (u'defendiendo_chile', 1),\n",
       " (u'vol_62', 4),\n",
       " (u'initial_production', 7),\n",
       " (u'buffalo_niagaras', 1),\n",
       " (u'flames_bas-reliefs', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = words.map(lambda value: value[1]).sum()\n",
    "words_countb = sc.broadcast(words_count)\n",
    "words_count_map = words.collectAsMap()\n",
    "words_count_mapb = sc.broadcast(words_count_map)\n",
    "\n",
    "pairs_count = pairs.map(lambda value: value[1]).sum()\n",
    "pairs_countb = sc.broadcast(pairs_count)\n",
    "pairs_count_map = pairs.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6971026 6966926 6971026 6966926\n",
      "43\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print words_count, pairs_count, words_countb.value, pairs_countb.value\n",
    "print words_count_map.get(\"ancyra\")\n",
    "print pairs_count_map.get(\"cases_federal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an intuition behind PMI\n",
    "\n",
    "PMI of two words, a & b, is defined as “PMI(a, b) = ln (P(ab) / (P(a) * P(b))”, where P(ab) is the probability of two words coming one after the other, and P(a) and P(b) are probabilities of words a & b respectively.\n",
    "\n",
    "You will estimate probabilities with occurrence counts, that is “P(a) = # of occurrences of word a / total number of words”, and “P(ab) = # of occurrences of words ‘a b’ / total number of word pairs”.\n",
    "\n",
    "To build an intuition behind the definition, consider the following cases:\n",
    "\n",
    "__“roman empire”__; assume that this is a unique combination, and every occurrence of “roman” is followed by “empire”, and, vice versa, every occurrence of “empire” is preceded by “roman”. In this case, “P(ab) = P(a) = P(b)”, so “PMI(a, b) = -ln P(a) = -ln P(b)”. This quantity increases when the probability of the collocation is low.\n",
    "\n",
    "__“the doors”__; let’s assume that “the” may occur with every word, independently. Thus, “P(ab) = P(a)*P(b)”, and “PMI(a, b) = ln 1 = 0”.\n",
    "\n",
    "__“green idea / sleeps furiously”__; when two words never occur together, “P(ab) = 0”, and “PMI(a, b) = -inf”. Therefore, rare combinations of coupled words have large PMI.\n",
    "\n",
    "If you want to deploy the environment on your own machine, please use [bigdatateam/spark-course1](https://hub.docker.com/r/bigdatateam/spark-course1/) Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npmi(value):\n",
    "    pair, count = value\n",
    "    w1, w2 = pair.split(\"_\")\n",
    "    w1_count = words_count_mapb.value[w1]\n",
    "    w2_count = words_count_mapb.value[w2]\n",
    "    \n",
    "    prob_pair = float(count) / pairs_countb.value\n",
    "    prob_w1 = float(w1_count) / words_countb.value\n",
    "    prob_w2 = float(w2_count) / words_countb.value\n",
    "    pmi = math.log(prob_pair / (prob_w1 * prob_w2))\n",
    "    npmi = pmi / (-1 * math.log(prob_pair))\n",
    "    return (pair, npmi)\n",
    "\n",
    "#PMI(a, b) = ln (P(ab) / (P(a) * P(b))\n",
    "#P(a) = # of occurrences of word a / total number of words\n",
    "#P(ab) = # of occurrences of words ‘a b’ / total number of word pairs\n",
    "#NPMI = NPMI(a, b) = PMI(a, b) / -ln P(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (pairs.filter(lambda value : value[1] > 500)\n",
    "            .map(lambda value: npmi(value))\n",
    "            .sortBy(lambda value: value[1], ascending=False)\n",
    "            ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los_angeles\t0.97\n",
      "external_links\t0.95\n",
      "united_states\t0.88\n",
      "prime_minister\t0.88\n",
      "san_francisco\t0.85\n",
      "et_al\t0.80\n",
      "new_york\t0.79\n",
      "supreme_court\t0.78\n",
      "19th_century\t0.76\n",
      "20th_century\t0.75\n",
      "references_external\t0.73\n",
      "soviet_union\t0.73\n",
      "air_force\t0.71\n",
      "baseball_player\t0.69\n",
      "university_press\t0.69\n",
      "roman_catholic\t0.68\n",
      "united_kingdom\t0.68\n",
      "references_reading\t0.67\n",
      "notes_references\t0.66\n",
      "award_best\t0.66\n",
      "north_america\t0.65\n",
      "new_zealand\t0.65\n",
      "civil_war\t0.64\n",
      "catholic_church\t0.63\n",
      "world_war\t0.62\n",
      "war_ii\t0.62\n",
      "south_africa\t0.62\n",
      "took_place\t0.61\n",
      "roman_empire\t0.61\n",
      "united_nations\t0.61\n",
      "american_singer-songwriter\t0.57\n",
      "high_school\t0.56\n",
      "american_actor\t0.56\n",
      "american_actress\t0.54\n",
      "american_baseball\t0.51\n",
      "york_city\t0.49\n",
      "american_football\t0.48\n",
      "years_later\t0.42\n",
      "north_american\t0.38\n"
     ]
    }
   ],
   "source": [
    "for key, value in result.take(39):\n",
    "    print (\"%s\\t%.2f\" % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'los_angeles', 0.9728997994856813), (u'external_links', 0.9496902298660744), (u'united_states', 0.8833319251733903), (u'prime_minister', 0.8827431167372655), (u'san_francisco', 0.8522919430661895), (u'et_al', 0.8025243317394334), (u'new_york', 0.7870688931579025), (u'supreme_court', 0.7781367974409652), (u'19th_century', 0.7574641661772452), (u'20th_century', 0.7514604533485757)]\n"
     ]
    }
   ],
   "source": [
    "print result.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
